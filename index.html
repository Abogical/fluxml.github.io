---
layout: default
---

<!-- Header -->
<div class="jumbotron jumbotron-fluid">
  <div class="container" style="text-align:center">
    <h1 class="display-3">Flux</h1>
    <h2>The <i>Elegant</i> Machine Learning Stack</h2>
    <hr class="my-4">
    <p class="lighter">
      Models that look like mathematics. Seamless derivatives, GPU training and deployment. A set of small, nimble tools that each do one thing and do it well.
    </p>
    <a class="btn btn-primary btn-lg" href="Flux.jl" role="button">Try It Out</a>
  </div>
</div>

<!-- Main Content -->
<div class="bg-light">
  <!-- Installation -->
  <div class="feature">
    <div class="container">
      <div class="row">
        <div class="col">
            <p>
              Installation? What installation? Just use Julia's package manager and you're done.
            </p>
        </div>
        <div class="col">
          <pre><code class="julia">Pkg.add("Flux")</code></pre>
        </div>
      </div>
    </div>
  </div>
  <!-- Models -->
  <div class="feature">
    <div class="container">
      <div class="row">
        <div class="col">
<pre><code class="julia">W = randn(2, 10)
b = randn(2)

y = σ(W * x .+ b)</code></pre>
        </div>
        <div class="col">
            <p>
              Where Python is executable pseudocode, Julia is executable math. Models look just like the description in the paper, and you have the full power and simplicity of the Julia language (including control flow, multiple dispatch and macros).
            </p>
        </div>
      </div>
    </div>
  </div>
  <!-- Hackability -->
  <div class="feature">
    <div class="container">
      <div class="row">
        <div class="col">
            <p>
              Flux is lightweight, and hackable to the core. The whole stack – including automatic differentiation and GPU kernels – is only a few thousand lines of clean Julia code. There's no monolithic C++ underbelly, so it's easy to use custom components and push the state of the art.
            </p>
        </div>
        <div class="col">
          <pre><code class="julia">function gpu_add(a, b, c)
  i = (blockIdx().x-1) * blockDim().x + threadIdx().x
  c[i] = a[i] + b[i]
  return nothing
end</code></pre>
        </div>
      </div>
    </div>
  </div>
  <!-- Compilation -->
  <div class="feature">
    <div class="container">
      <div class="row">
        <div class="col">
<pre><code class="julia">model = Chain(
  Dense(10, 5, σ),
  Dense(5, 2),
  softmax)</code></pre>
        </div>
        <div class="col">
            <p>
              Despite being simple and flexible, Flux has street smarts. The tools are graph-aware and can make smart optimisations to speed and memory usage, previously only possible in clunky "define before run" frameworks.
            </p>
        </div>
      </div>
    </div>
  </div>

  <div class="container call">
    <p class="lead">If your appetite is whet, <a href="Flux.jl">check out the docs</a> to get going.</p>
  </div>
</div>
