---
title: ANN: Flux v0.12 with faster Julia RNNs, faster GPU kernels, 1.6 support and more
author: Dhairya Gandhi
layout: blog
---

The new Flux v0.12 is out and available with a number of new features.

### CUDNN and CUDA changes

Some big changes the GPU stack mean that we should see a more complete coverage of Nvidia's CUDNN interface. This allows of unlocking some higher performant kernels as well to be used by default. The `softmax` function which can be compute heavy in some cases should now see faster runtime performance. There has also been speedups made to several other kernels in the CUDA side to have up to 3x faster ResNet style convolutional models.

### RNNs in Julia

The new RNN interface now relies on Julia code all the way through rather than using the CUDNN kernels. This gives us more flexiblity in the design moving on while being faster than the CUDNN implementation.

### Datasets moving out of Flux

While its very convenient to have simple toy datasets available through Flux, several packages in the Julia ecosystem have matured and come a long way to provide these and more comprehensive datasets, often in a manner compatible with the Flux API, therefore, starting v0.12 we have decided to deprecate the toy datasets from Flux and link to packages such as MLDatasets.jl.

### Zygote and IRTools now work with Julia 1.6!

Zygote and IRTools have been updated to work with Julia 1.6 out of the box (shoutout to Simeon!).
This is available with Zygote v0.6 onwards.

### Some Other updates:
* Zygote is made PackageCompiler friendly to be able to compile projects using it into system images to get rid of compilation overhead.
* New Optimisers - OADAM, AdaBelief
* Added label smoothing so it can be applied to supported loss functions
* Zygote supports parallelism via `pmap`
* OneHot{Vector/ Matrix} has been generalized as a `OneHotArray` and fixes the case of scalar indexing with GPUs in certain kernels.
* Experimentally - `outdims` (renamed to `outsize`) uses a custom `Nil` type to help with performance and correctness issues and be more general.
* NNlib `\nablasoftmax`, `\nablalogsoftmax` optionally accept a third argument of the output of `softmax` to avoid repeating the forward pass.
* train/testmode is now generalized to all Functors
* Added `Flux.skip` to optionally not optimise when conditions specified in a callback are met.
* ADAM is made more efficient by removing a large number of allocations.
* Added `kaiming_uniform/normal` initialization options.

More detailed updates can be found on the [GitHub releases page](https://github.com/FluxML/Flux.jl/releases/tag/v0.12.0)
